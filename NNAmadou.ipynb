{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT PERFORMANCES\n",
    "## Machine Learning project\n",
    "## Team: Amadou and Jamie\n",
    "\n",
    "* Analysis of student performances\n",
    "* Features: 33 \n",
    "* Data: 649\n",
    "\n",
    "There are 2 different csv files in the data folder. One (student-port.csv) being the data set of the students that took the portugese language course and another (student-mat.csv) of the students that are taking the math course. There are 382 students that are taking both courses. \n",
    "\n",
    "More information about the dataset is in the data folder, labeled student.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries to be used:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data\\student-por.csv\",sep=\";\") #read file from different directory\n",
    "df2 = pd.read_csv(\"data\\student-mat.csv\", sep = \";\")\n",
    "print(df1)\n",
    "#df1.to_csv(\"port.csv\") # made in to a csv after being made in to a table - easier to read compared to original excel file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data accordingly\n",
    "X = df1.iloc[:, :30]\n",
    "y = df1.iloc[:, 32:]\n",
    "X.dropna()\n",
    "y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixingfile(X):\n",
    "    switch = {0: (\"GP\", \"MS\"), 1: (\"F\", \"M\"), 3: (\"U\", \"R\"), 4: (\"LE3\", \"GT3\"), 5:(\"T\", \"A\"), 15:(\"yes\",\"no\"), 16:(\"yes\",\"no\"),\n",
    "             17:(\"yes\",\"no\"), 18:(\"yes\",\"no\"), 19:(\"yes\",\"no\"), 20:(\"yes\",\"no\"), 21:(\"yes\",\"no\"), 22:(\"yes\",\"no\")}\n",
    "    #ignore = [\"age\", \"Medu\", \"Fedu\",\"Mjob\", \"Fjob\", 'reason', 'guardian','traveltime', 'studytime',\n",
    "    #       'failures']\n",
    "    #print(X.columns)\n",
    "    #print(X.iloc[0,:])\n",
    "    for column in range(len(X.columns)):\n",
    "        if column not in switch:\n",
    "            continue\n",
    "        for i in range(len(X.iloc[:,column])):\n",
    "           #print(X.iloc[i, column] == \"U\",X.iloc[i, column], switch[column])\n",
    "            if X.iloc[i,column] == switch[column][0]:\n",
    "                #print(X.iloc[i, column], switch[column][0])\n",
    "                X.iloc[i,column] = 1\n",
    "            elif X.iloc[i, column] == switch[column][1]:\n",
    "                X.iloc[i,column] = 0\n",
    "    X.drop([\"Mjob\", \"Fjob\", \"reason\", \"guardian\"], axis = 1, inplace=True) #Remove the ones with more than 1 and 0 answers\n",
    "    print(X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fixed = fixingfile(X)\n",
    "y = np.array(y) #changed to array to be easier to work with\n",
    "X = np.array(X_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM here on out the markdown split up which model. Add cells according to which mark down. Here is the order\n",
    "## 1. Linear Regression\n",
    "## 2. Neutral Network \n",
    "## 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping y to 2d\n",
    "y_2d = y.reshape(y.shape[0], 1)\n",
    "print(\"Checking y_2d is in 2d:\", y_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = y.shape[0]\n",
    "print(\"Number of rows:\", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the column of ones infront of x\n",
    "ones = np.ones((N, 1))\n",
    "X_1 = np.hstack((ones, X))\n",
    "print(\"X_1 shape:\", X_1)\n",
    "print(\"X_1 with ones: \", X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X_1, y_2d, w, N):\n",
    "    # Write your code in place of ellipsis. Cost can be calculated using a single line of code.\n",
    "    # Remember w is a vector here.\n",
    "    \n",
    "    # TODO Q07 - Taken from hw\n",
    "    # Write the cost function\n",
    "    #ISSUES: X AND Y ARE NOW 2D ARRAYS\n",
    "    cost = sum((y_2d- (np.dot(X_1, w)))**2)/(2*N)\n",
    "    \n",
    "    return cost[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_1, y_2d, learning_rate, w, N, num_iters):\n",
    "    # In place of ellipsis, write the updated value of w0 in temp0 and of w1 in temp1\n",
    "    # TODO Q08\n",
    "    # Finish the gradient descent function\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # derivative vector is given by : X_train.Transpose *  (( X_train * w)- y ) \n",
    "        slope = np.dot(X_1, w)\n",
    "        der_vector = np.dot(X_1.T,(slope - y_2d))\n",
    "        w = w - (learning_rate * (1/N)) * der_vector\n",
    "       \n",
    "        \n",
    "        if(i % 100 == 0):\n",
    "            # In place of ellipsis, call the cost you just coded above\n",
    "            cost = compute_cost(X_1, y_2d, w, N)\n",
    "            # You can uncomment print statements below to see how cost changes, \n",
    "            # but please make sure you put prints in comments before submission\n",
    "            #print(\"Cost\")\n",
    "            #print(cost)\n",
    "             \n",
    "    return w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_testcase = np.zeros((X.shape[1]+1,1))\n",
    "g = gradient_descent(X_1, y_2d, 0.0049, w_testcase, N, 1000)\n",
    "#print(g)\n",
    "print(\"g[0]: \", g[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_linear_reg_model_gda(X_1, y_2d, learning_rate, N, num_iters):\n",
    "    #initialize the values of parameter vector w. It should be a column vector of zeros of dimension(d+1,1)\n",
    "    # TODO 09 \n",
    "    # Complete the function\n",
    "    w = np.zeros((X_1.shape[1],1))\n",
    "    \n",
    "    # Calculate the initial cost by calling the function you coded above.\n",
    "    initial_cost = compute_cost(X_1, y_2d, w, N)\n",
    "    \n",
    "    # Calculate the optimized value of gradients by calling the gradient_descent function coded above\n",
    "    w = gradient_descent(X_1, y_2d, learning_rate, w, N, num_iters)\n",
    "    \n",
    "    # Calculate the cost with the optimized value of w0 and w1 by calling the cost function.\n",
    "    final_cost = compute_cost(X_1, y_2d, w, N)\n",
    "\n",
    "    return w, initial_cost, final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0049\n",
    "num_iters = 100\n",
    "results = multiple_linear_reg_model_gda(X_1, y_2d, learning_rate, N, num_iters)\n",
    "print(\"w: \", results[0], \"\\ninitial cost: \", results[1], \"\\nfinal cost: \", results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale = StandardScaler()\n",
    "X_fit= np.array(X_scale.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and test set.  60% training and %40 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fit, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code copied from hw6, but edited to match our y values\n",
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 20))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)\n",
    "print(\"Testing training set shape: \",y_v_train.shape)\n",
    "print(\"Testing testing set shape: \",y_v_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function - depends on whether we want to use it or not\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def f_deriv(z):\n",
    "    return f(z) * (1 - f(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out) * f_deriv(z_out) \n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train NN without regularization\n",
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l])) \n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rigde_nn(nn_structure, X, y, iter_num=3000, alpha=0.25, hyper = 0.1):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l])) \n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l]) + hyper * W[l]\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
