{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDENT PERFORMANCES\n",
    "## Machine Learning project\n",
    "## Team: Amadou and Jamie\n",
    "\n",
    "* Analysis of student performances\n",
    "* Features: 33 \n",
    "* Data: 649\n",
    "\n",
    "There are 2 different csv files in the data folder. One (student-port.csv) being the data set of the students that took the portugese language course and another (student-mat.csv) of the students that are taking the math course. There are 382 students that are taking both courses. \n",
    "\n",
    "More information about the dataset is in the data folder, labeled student.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries to be used:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as r\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
      "0       GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
      "1       GP   F   17       U     GT3       T     1     1   at_home     other   \n",
      "2       GP   F   15       U     LE3       T     1     1   at_home     other   \n",
      "3       GP   F   15       U     GT3       T     4     2    health  services   \n",
      "4       GP   F   16       U     GT3       T     3     3     other     other   \n",
      "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
      "644     MS   F   19       R     GT3       T     2     3  services     other   \n",
      "645     MS   F   18       U     LE3       T     3     1   teacher  services   \n",
      "646     MS   F   18       U     GT3       T     1     1     other     other   \n",
      "647     MS   M   17       U     LE3       T     3     1  services  services   \n",
      "648     MS   M   18       R     LE3       T     3     2  services     other   \n",
      "\n",
      "     ... famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
      "0    ...      4        3      4     1     1      3        4   0  11  11  \n",
      "1    ...      5        3      3     1     1      3        2   9  11  11  \n",
      "2    ...      4        3      2     2     3      3        6  12  13  12  \n",
      "3    ...      3        2      2     1     1      5        0  14  14  14  \n",
      "4    ...      4        3      2     1     2      5        0  11  13  13  \n",
      "..   ...    ...      ...    ...   ...   ...    ...      ...  ..  ..  ..  \n",
      "644  ...      5        4      2     1     2      5        4  10  11  10  \n",
      "645  ...      4        3      4     1     1      1        4  15  15  16  \n",
      "646  ...      1        1      1     1     1      5        6  11  12   9  \n",
      "647  ...      2        4      5     3     4      2        6  10  10  10  \n",
      "648  ...      4        4      1     3     4      5        4  10  11  11  \n",
      "\n",
      "[649 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"data\\student-por.csv\",sep=\";\") #read file from different directory\n",
    "print(df1)\n",
    "#df1.to_csv(\"port.csv\") # made in to a csv after being made in to a table - easier to read compared to original excel file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       7.7\n",
       "1      10.4\n",
       "2      12.3\n",
       "3      14.0\n",
       "4      12.4\n",
       "       ... \n",
       "644    10.3\n",
       "645    15.4\n",
       "646    10.5\n",
       "647    10.0\n",
       "648    10.7\n",
       "Length: 649, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting the data accordingly\n",
    "X = df1.iloc[:, :30]\n",
    "y = 0.3 * df1.iloc[:, 30] + 0.3 * df1.iloc[:, 31] + 0.4 * df1.iloc[:, 32]\n",
    "X.dropna()\n",
    "y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixingfile(X):\n",
    "    switch = {0: (\"GP\", \"MS\"), 1: (\"F\", \"M\"), 3: (\"U\", \"R\"), 4: (\"LE3\", \"GT3\"), 5:(\"T\", \"A\"), 15:(\"yes\",\"no\"), 16:(\"yes\",\"no\"),\n",
    "             17:(\"yes\",\"no\"), 18:(\"yes\",\"no\"), 19:(\"yes\",\"no\"), 20:(\"yes\",\"no\"), 21:(\"yes\",\"no\"), 22:(\"yes\",\"no\")}\n",
    "    #ignore = [\"age\", \"Medu\", \"Fedu\",\"Mjob\", \"Fjob\", 'reason', 'guardian','traveltime', 'studytime',\n",
    "    #       'failures']\n",
    "    #print(X.columns)\n",
    "    #print(X.iloc[0,:])\n",
    "    for column in range(len(X.columns)):\n",
    "        if column not in switch:\n",
    "            continue\n",
    "        for i in range(len(X.iloc[:,column])):\n",
    "           #print(X.iloc[i, column] == \"U\",X.iloc[i, column], switch[column])\n",
    "            if X.iloc[i,column] == switch[column][0]:\n",
    "                #print(X.iloc[i, column], switch[column][0])\n",
    "                X.iloc[i,column] = 1\n",
    "            elif X.iloc[i, column] == switch[column][1]:\n",
    "                X.iloc[i,column] = 0\n",
    "    X.drop([\"Mjob\", \"Fjob\", \"reason\", \"guardian\"], axis = 1, inplace=True) #Remove the ones with more than 1 and 0 answers\n",
    "    print(X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649, 26)\n"
     ]
    }
   ],
   "source": [
    "X_fixed = fixingfile(X)\n",
    "y = np.array(y) #changed to array to be easier to work with\n",
    "X = np.array(X_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM here on out the markdown split up which model. Add cells according to which mark down. Here is the order\n",
    "## 1. Linear Regression\n",
    "## 2. Neutral Network \n",
    "## 3. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 649\n"
     ]
    }
   ],
   "source": [
    "N = y.shape[0]\n",
    "print(\"Number of rows:\", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale = StandardScaler()\n",
    "X_fit= np.array(X_scale.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and test set.  60% training and %40 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fit, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False  True  True  True False  True  True  True\n",
      " False False False  True  True  True  True  True  True  True  True False\n",
      " False  True False  True False False  True  True False  True  True  True\n",
      " False  True  True  True  True  True False  True  True False  True  True\n",
      "  True False  True  True  True  True False  True False  True False False\n",
      "  True  True False  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True False  True  True  True False  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True False  True  True  True False  True  True  True  True  True  True\n",
      "  True False False  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True False False False False  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True False False  True False  True  True False  True  True  True\n",
      "  True False False  True  True False  True  True  True  True  True  True\n",
      "  True False  True False False False False  True False False  True  True\n",
      "  True  True False  True  True False  True  True  True  True False  True\n",
      "  True  True False  True False  True  True  True  True  True  True False\n",
      "  True  True  True False False  True False  True  True False  True  True\n",
      "  True  True False  True False False  True  True]\n",
      "[ True  True  True False False  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False  True  True  True\n",
      "  True False  True  True False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True False\n",
      "  True  True False  True False  True  True  True  True  True False False\n",
      " False False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True False  True  True  True False  True\n",
      "  True  True  True  True False  True False  True  True False  True  True\n",
      "  True  True  True  True  True  True  True  True False False  True  True\n",
      "  True False False  True  True  True  True  True  True False False  True\n",
      " False  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True False  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False  True  True False False  True  True  True  True  True  True  True\n",
      "  True False  True False False  True  True False False False  True  True\n",
      "  True  True  True False  True  True  True  True  True False  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True False False  True  True  True  True False  True  True\n",
      "  True  True False  True  True  True False  True  True  True  True  True\n",
      "  True  True  True False  True False False False  True False  True False\n",
      "  True False  True False  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True False  True\n",
      " False  True  True  True  True  True  True  True False  True False  True\n",
      " False  True  True  True False  True  True False False  True  True False\n",
      "  True False False False  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True False False False  True  True  True  True\n",
      "  True  True  True  True False  True  True  True False  True  True  True\n",
      "  True  True False False  True  True  True  True  True  True False  True\n",
      "  True  True  True False False  True False  True  True  True  True  True\n",
      " False  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(y_test >= 10)\n",
    "print(y_train >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 2))\n",
    "    for i in range(len(y)):\n",
    "        if y[i] >= 10:\n",
    "            y_vect[i, 0] = 1\n",
    "        else:\n",
    "            y_vect[i, 1] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "training set shape:  (389, 2)\n",
      "testing set shape:  (260, 2)\n"
     ]
    }
   ],
   "source": [
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)\n",
    "print(y_v_train)\n",
    "print(\"training set shape: \",y_v_train.shape)\n",
    "print(\"testing set shape: \",y_v_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function - depends on whether we want to use it or not\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def f_deriv(z):\n",
    "    return f(z) * (1 - f(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out) * f_deriv(z_out) \n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} #creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) #Return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        a[l+1] = f(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train NN without regularization\n",
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l])) \n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rigde_nn(nn_structure, X, y, iter_num=3000, alpha=0.25, hyper = 0.1):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l])) \n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/N * tri_W[l]) + hyper * W[l]\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete they_test average cost calculation\n",
    "        avg_cost = 1.0/N * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n",
      "Iteration 0 of 3000\n",
      "Iteration 1000 of 3000\n",
      "Iteration 2000 of 3000\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [26, 30, 2]\n",
    "itter = [1000,2000,3000,4000,5000]\n",
    "# train the NN\n",
    "\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the avg_cost_func\n",
    "plt.plot(avg_cost_func)\n",
    "plt.ylabel('Average J')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.  12.7 13.4 10.1  8.7 11.  13.3 18.7  8.4 12.7 13.3 15.8  8.7  9.7\n",
      "  7.7 13.7 12.3 17.3 14.4 12.8 13.6 14.7 10.   6.4  8.4 13.7  9.7 12.5\n",
      "  9.8  7.7 13.  17.4  8.4 10.3 13.4 11.7  9.4 13.3 15.  12.  12.3 11.1\n",
      "  5.7 10.  18.   6.7 12.9 12.7 15.3  9.7 10.4 14.4 15.3 11.7  9.4 10.1\n",
      "  8.6 17.1  8.7  8.6 13.1 11.   9.7 14.4 11.4 13.3 14.4 10.5 13.   9.4\n",
      " 14.   8.6 11.7 15.1 13.3 10.7 15.7  9.  15.  10.7 15.4  7.4 13.7 13.2\n",
      " 11.3 10.3 13.7 15.5 10.4 10.7 12.1  9.7 15.1 11.3 11.6 11.4 11.1 15.4\n",
      " 16.4 12.4 10.7 11.7 12.  12.4  8.3 15.7 13.  15.8 10.3  9.1 12.4 10.\n",
      " 13.3  9.7 11.7 15.  13.7 17.7 13.1 15.7 11.1  9.4  9.1 14.1 11.3  7.4\n",
      " 10.7 12.4 12.1 17.  11.1 12.1 10.  15.  13.7 15.1 11.  12.  10.4 10.3\n",
      " 10.7 15.7 12.7 11.  15.  18.   7.7 14.3  9.   8.   9.4  7.4 14.7 13.8\n",
      "  7.  13.1 13.  11.7 13.  11.8 11.6 12.7 13.1 10.1 14.7 10.  10.  10.1\n",
      "  8.7 10.1 10.3 11.3 14.  12.3 14.1 12.7  5.8 12.7 11.1 13.  11.3 12.3\n",
      "  8.7  8.7 11.   8.4 16.  14.4  9.4 17.1 13.  12.8 13.3  7.3  9.1 15.8\n",
      " 14.7  8.  11.4 14.1 15.5 10.3 11.  16.4 11.4  1.5 10.   7.3  8.1  9.\n",
      "  8.8 10.   8.3  8.4 12.3 12.  17.4 12.   9.7 11.  14.7  3.6 11.3 12.1\n",
      " 10.4 14.7  9.7 12.3 16.3 12.4  9.3 12.8  8.  10.  11.3 15.4 17.7 10.\n",
      " 10.6  9.8 14.  14.4 16.7  7.1  8.8 11.8  9.7 10.7 14.   9.4 14.7 12.2\n",
      " 10.7 16.4  2.7 11.   8.7  9.4 14.8 10.4]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False  True  True  True False  True  True  True\n",
      " False False False  True  True  True  True  True  True  True  True False\n",
      " False  True False  True False False  True  True False  True  True  True\n",
      " False  True  True  True  True  True False  True  True False  True  True\n",
      "  True False  True  True  True  True False  True False  True False False\n",
      "  True  True False  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True False  True  True  True False  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True False  True  True  True False  True  True  True  True  True  True\n",
      "  True False False  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True False False False False  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True False False  True False  True  True False  True  True  True\n",
      "  True False False  True  True False  True  True  True  True  True  True\n",
      "  True False  True False False False False  True False False  True  True\n",
      "  True  True False  True  True False  True  True  True  True False  True\n",
      "  True  True False  True False  True  True  True  True  True  True False\n",
      "  True  True  True False False  True False  True  True False  True  True\n",
      "  True  True False  True False False  True  True]\n",
      "260\n",
      "Prediction accuracy is 16.538461538461537%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "y_test = y_test >= 10\n",
    "print(y_test)\n",
    "print(len(y_pred))\n",
    "\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True False False  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False  True  True  True\n",
      "  True False  True  True False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True False\n",
      "  True  True False  True False  True  True  True  True  True False False\n",
      " False False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True False  True  True  True False  True\n",
      "  True  True  True  True False  True False  True  True False  True  True\n",
      "  True  True  True  True  True  True  True  True False False  True  True\n",
      "  True False False  True  True  True  True  True  True False False  True\n",
      " False  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True False  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False  True  True False False  True  True  True  True  True  True  True\n",
      "  True False  True False False  True  True False False False  True  True\n",
      "  True  True  True False  True  True  True  True  True False  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True False False  True  True  True  True False  True  True\n",
      "  True  True False  True  True  True False  True  True  True  True  True\n",
      "  True  True  True False  True False False False  True False  True False\n",
      "  True False  True False  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True False  True\n",
      " False  True  True  True  True  True  True  True False  True False  True\n",
      " False  True  True  True False  True  True False False  True  True False\n",
      "  True False False False  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True False False False  True  True  True  True\n",
      "  True  True  True  True False  True  True  True False  True  True  True\n",
      "  True  True False False  True  True  True  True  True  True False  True\n",
      "  True  True  True False False  True False  True  True  True  True  True\n",
      " False  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True]\n",
      "training accuracy is 10.539845758354756%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(W, b, X_train, 3)\n",
    "y_train = y_train >= 10\n",
    "print(y_train)\n",
    "print('training accuracy is {}%'.format(accuracy_score(y_train, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n",
      "Iteration 0 of 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-0879e0f3e655>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [26, 30, 2]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_rigde_nn(nn_structure, X_train, y_v_train, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W,b)\n",
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "y_test = y_test >= 10\n",
    "print(y_test)\n",
    "print(y_pred)\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
